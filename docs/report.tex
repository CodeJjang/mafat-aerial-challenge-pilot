\documentclass[]{article}
\usepackage{graphicx}

%opening
\title{Mafat (DDR\&D) Challenge\\
	Detection and fine grained classification of objects in aerial imagery
	}
\author{Aviad Moreshet\\Bar Ilan University}

\begin{document}
\date{June 21, 2018}
\maketitle
\begin{abstract}
Object detection is an important and challenging problem in computer vision field. Despite the major advances in detection of objects in natural scenes, not much work has been done on aerial imagery scenes. Those scenes are characterized by a unique bird's eye view, tilted orientation, may contain up to dozens or hundreds of objects with a relatively small objects, etc.
In this paper I tried to solve Mafat Challenge of Detection and fine-grained classification of objects in aerial imagery, by training and fine-tuning several ConvNets architectures on the published dataset.

\end{abstract}



\section{The Task}
There are mainly four types of tasks in the field of Object Recognition in Computer Vision: Object Classification and Localization, in which we need to classify whether an object appears in an image or not, along with a surrounding bounding box. Object Detection, which is like the previous task but now multiple objects are allowed per image. Semantic Segmantation, in which we need to label (usually by coloring) each pixel in an image with a category label, and finally Instance Segmentation, in which we need detect multiple objects per image and also color (label) their exact pixels.\\
In this challenge, the task is in the Object Detection field. More precisely, we need to detect 4 coarse grained classes and perform fine grained classification of objects in aerial imagery.\\
The coarse grained classes are Small Vehicle, Large Vehicle, Solar Panel and Utility Pole. Small and Large Vehicle classes have further fine grained features we need to detect, as following:\\
\begin{enumerate}
\item Small Vehicle
	\begin{enumerate}
		\item Subclasses: Sedan, Hatchback, Minivan, Van, Pickup truck, Jeep, Public vehicle.
		\item Features: Sunroof, Taxi, Luggage carrier, Open cargo area, enclosed cab, Wrecked, Spare wheel.
		\item Colors: Yellow, Red, Blue, Black, Silver/Grey, White, Other.
	\end{enumerate}
\item Large Vehicle
	\begin{enumerate}
	\item Subclasses: Truck, Light Truck, Concrete mixer truck, Dedicated Agricultural vehicle, Crane truck, Prime mover, Tanker, Bus, Minibus.
	\item Features: Open cargo area, Vents, Air conditioner, Wrecked, Enclosed box, Enclosed cab, Ladder, Flatbed, Soft shell box, Harnessed to a cart.
	\item Colors: Yellow, Red, Blue, Black, Silver/Grey, White, Other.
	\end{enumerate}
\end{enumerate}
In the detection of Subclass and Colors classes, each detection should include a single value, while the Features detection has to include multiple values (has sunroof? has luggage carrier? etc).

\section{Evaluation}
The final score will be a multiplication of the Coarse Grained classes detection score and the Fine Grained classes classification score.

\subsection{Coarse Grained Score}
Here we use the popular Mean Average Precision score. First, we have to define TP and FP of a bounding box prediction. TP will be defined by the IoU (intersection area over union area) score. If the IoU between observed Bouding Box and predicted Bounding Box is larger than 0.25, we mark this detection as TP. For Utility Poles which are labeled by 1 point, if the distance between observed and predicted point is less than 30 pixels, we consider the detection as TP as well. Otherwise, a detection is marked as FP. For multiple detections, only the first detection is considered TP. The rest will be considered FP. Now, we can calculate AP:\\
$$AP(class)=\frac{1}{K}\sum_{k=1}^{K}precision(k)rel(k)$$
$$mAP=\frac{1}{N_c}\sum_{class=1}^{N_c}AP(class)$$
Where $K$ is the total number of objects from the class in the test data, $precision(k)$ is the precision calculated over the first k objects, $rel(k)$ is 1 if object k is true, else 0 and $N_c$ is the number of categories.

\subsection{Fine Grained Score}
Calculated only for Coarse Grained classes that were predicted correctly.
The challenge suggested to calculate the score using Cohen's Kappa score, however, due to lack of time to classify all the Fine Grained features, I decided to evaluate my results (of the several classes I did classify) with a normal accuracy (precision) evaluation.

\section{The Dataset}
\subsection{Brief}
The dataset contains thousands of aerial images, taken from diverse geographical locations, different times, resolutions, area coverage and photo conditions (weather, angles and lighting). The resolutions vary between 5cm to 15cm Ground Sample Distance (the distance between pixel centers measured on the ground). Some samples (generated with VoTT \cite{vott}):\\
\begin{figure}[!h]
\centering
\includegraphics[width=1\linewidth]{"images/im1"}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[width=1\linewidth]{"images/im2"}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[width=1\linewidth]{"images/im3"}
\end{figure}

\clearpage
\subsection{Analysis}
Most of the dataset does not contain objects at all. Only 51 images are fully tagged. 2161 images are partially tagged, 7121 images were verified to contain no objects at all, and 36 images are untagged.\\
Regarding labels, the dataset definitely biases towards small vehicles, which appear more than the other classes, combined. Distribution can be seen in Figure 1.
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{"charts/Dataset Coarse Grained Labels Distribution"}
\caption{Dataset Coarse Grained Labels Distribution}
\label{fig:Dataset Coarse Grained Labels Distribution}
\end{figure}
As mentioned before, large and small vehicles are further tagged with fine-grained labels, consisted of 3 categories: subclass, feature and color.\\
The large vehicle class has only hundreds of subclass tags, while small vehicle has many thousands, as seen in Figure 2.
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{"charts/Dataset Subclass Labels Distribution"}
\caption{Dataset Subclass Labels Distribution}
\label{fig:Dataset Subclass Labels Distribution}
\end{figure}
The same goes with feature tags, as seen in Figure 3,
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{"charts/Dataset Feature Labels Distribution"}
\caption{Dataset Feature Labels Distribution}
\label{fig:Dataset Feature Labels Distribution}
\end{figure}
and also with color tags, as seen in Figure 4.
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{"charts/Dataset Color Labels Distribution"}
\caption{Dataset Color Labels Distribution}
\label{fig:Dataset Color Labels Distribution}
\end{figure}
\\
On average, there are 8 objects in an image, while the most crowded image contains 164 objects altogether.\\

In Figure 5 we can see just how small are our objects, and what is their frequency in terms of square rooted area. The large vehicle objects spans roughly on all the size range, with more appearances around sizes 40-100. Small vehicle objects, however, mostly appear in sizes 20-70. Lastly, our solar panel objects are relatively very small, ranging around sizes 10-40.
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{"charts/Dataset Objects Size Distribution"}
\caption{Dataset Objects Size Distribution}
\label{fig:Dataset Objects Size Distribution}
\end{figure}

\subsection{Dataset splits}
I chose to randomly select 90\% of the dataset as training set and the rest 10\% as validation set. Test set should be supplied by the competition organizers.\\
A small percentage of validation set could impose a set which does not distribute as the training set, resulting in worse results in test time.\\
However, I've still decided to stick with this split since there is no much data and due to the fact that I'm going to train on the full dataset anyway before submitting results (in order to slightly increase the accuracies).

\subsection{Dataset formats}
I wrote a converter of the dataset into ImageNet \cite{imagenet} and Pascal VOC \cite{pascalvoc} formats in order to feed it to the neural networks which support only popular well-known formats.

\section{Chosen Neural Networks}
Following the DOTA paper \cite{dota} in which Faster-RCNN \cite{fasterrcnn} achieved best scores on the detection task on an aerial dataset, I decided to use it on my aerial dataset as well. The F-RCNN \cite{fasterrcnn} backbone network was an ImageNet \cite{imagenet} pretrained ResNet-101 \cite{resnet}. I used Jianwei Yang's implementation \cite{jianweiyang} of this architecture in PyTorch \cite{pytorch}.\\
For classification of fine-grained features I used an ImageNet \cite{imagenet} pretrained ResNet-50 \cite{resnet}, which should give a good trade-off between speed and accuracy (ResNet-101 and ResNet-152 would give a little more accuracy but take longer time to train). I used the default PyTorch \cite{pytorch} implementation for this architecture.

\section{Challenges}
1. Oriented Bounding Box \\
The dataset consisted of Oriented Bounding Boxes (Bounding Boxes which are not axis aligned). \\
The popular dataset formats however support axis aligned Bounding Boxes only, called Horizontal Bounding Boxes. 
In addition, OBBs are more difficult to learn than HBBs since they are more strict. Hence I decided to wrap each OBB in an HBB, essentially making all Bounding Boxes axis aligned.  
\\\\
2. Classes annotated with a single point\\
The utility pole class objects are annotated with a single point instead of 4 points Bounding Box. Therefore, in the initial experiments I've decided to remove all the utility poles objects from the dataset (and also images which contained only utility poles, 533 in total), as they are extremely difficult to detect and posed several problems in the training. Later, I added them to the training data by extending each point to a 1x1 bounding box.
\\\\
3. Fine Grained features\\
Fine-grained features classification is not part of the standard detection ConvNets implementations, which usually classify only one set of classes.
In order to allow the F-RCNN \cite{fasterrcnn} to learn fine-grained features, I decided to integrate several Feed Forward layers on top of the network which would learn each of the fine-grained features, starting from the color feature.
Trying to integrate the additional layers in the original network implementation were difficult, and were accompanied by weeks of debugging the network and trying to figure why it doesn't learn the fine-grained features, or worse, why does it badly ruin the accuracy of other coarse-grained classes.\\
I had to solve many size mismatch bugs, re-compile some Cuda low level modules which contained hard-coded tensor sizes, etc.
I have documented the results of those trials but did not add them to my experiments list as they have yielded really low accuracies.\\
I realized that perhaps the network doesn't learn the fine-grained features along with the coarse-grained ones because in order to learn a good representation for the coarse-grained classes, it omits many details (such as color) for this task. To prove this, I trained a separate ResNet-50 \cite{resnet} for the color fine-grained feature, which gave good results supporting my assumption. I hereby decided to take this route and train this network for several more fine-grained features.
\\\\
4. Falsely annotated images\\
Some image annotations contained bounding boxes with coordiantes outside of the image boundaries. I decided to crop these coordinates to fit inside the images, which is better in my opinion than entirely remove them from the dataset.
\\\\
5. Small objects\\
The objects appeared in the images are much smaller than standard objects networks usually try to detect. Therefore, the Region Proposal Network of the F-RCNN \cite{fasterrcnn} missed all the objects. I had to fine-tune some RPN related hyper-parameters in order to be able to detect the small objects in the dataset. The most important parameter was to set anchor scales to [1, 2, 4, 8, 16], which helped covering various objects sizes.
\\\\
6. High resolution images\\
About 100 images in the dataset have relatively high resolutions reaching up to 4k. Including those images in the dataset required scaling them down, since they wouldn't fit into GPU memory.

\section{Experiments}
I mostly used default learning-related hyper parameters in the following experiments, and fine-tuned the hyper parameters which would help the network to better learn the small sized objects of the dataset. There are more than 30 hyper parameters, so I'll mention the most important ones, and of course each experiment will describe the hyper parameters tuned for the experiment.\\
For the F-RCNN \cite{fasterrcnn}, learning rate was 0.001, optimizer was SGD with 0.9 momentum, weight decay was 5e-4, epochs number was 15, RPN anchor ratios were [0.5,1,2] and RPN feature stride was 16.\\
As for the ResNet-50 \cite{resnet}, batch size was 256, learning rate 0.1, momentum was 0.9 and weight decay was 1e-4. The network were trained for 90 epochs. None of those parameters were changed in the experiments.
\\\\
The initial experiments results (before I found the baseline hyper paremeters setup) are omitted, because the network didn't even train. For the same reason, experiments which contained bugs (due to the error and trial nature of the process) are also not metioned.
\\\\
I also did not see reasons to collect training plots regarding loss and accuracies until I started optimizing them, since the first upcoming experiments have other targets.

\subsection{Experient 1}
This is the first experiment which yielded actual results.\\
I used only images up to 1k resolution, with 2 classes: Small Vehicle and Large Vehicle (Solar Panel did not appear in filtered dataset).
Scale used was 600 (which didn't basically affect anything since all the images were around 1000x600), and batch size was 2.\\
Results:\\
Large Vehicle AP = 0.7531\\
Small Vehicle AP = 0.8962\\
mAP = 0.82465.\\

\subsection{Experient 2}
In this experiment the exact same parameters were used, but now the training consisted of all the images. The scale parameter now took effect and actually rescaled all the large images small side to be 600 (i.e. 4000x2500 image would now be 960x600).\\
Results:\\
Large Vehicle AP = 0.3520\\
Small Vehicle AP = 0.7978\\
Solar Panel AP = 0.0529\\
mAP = 0.4009

\subsection{Experiment 3}
Again, same parameters were used, but now I wanted to test the effect of the scale size on the learning. Therefore, I used the maximum scale size I could (due to 12GB GPU memory restriction), which was 1600.\\
Results:\\
Large Vehicle AP = 0.4987\\
Small Vehicle AP = 0.8917\\
Solar Panel AP = 0.6431\\
mAP = 0.6778

\subsection{Experiment 4}
I now wanted to test the combination the scale parameter values, thus, I set the scale size to be [600, 1600] (meaning each image would be scaled twice, by the two factors). All other parameters remained the same.\\
Results:\\
Large Vehicle AP = 0.3340\\
Small Vehicle AP = 0.7906\\
Solar Panel AP = 0.1700\\
mAP = 0.4316

\subsection{Experiment 5}
I've returned to Experiment 3 parameters, and added the Utility Pole class to the training, but generating a 1x1 Bounding Box for each Utility Pole point.
Results:\\
Large Vehicle AP = 0.5416\\
Small Vehicle AP = 0.9049\\
Solar Panel AP = 0.4545\\
Utility Pole AP = 0.2363\\
mAP = 0.5343

\subsection{Experiment 6}
I now trained a ResNet-50 \cite{resnet} network from scratch for classifying the color fine-grained feature of Small and Large Vehicles.\\
For this task I cropped all tagged Small and Large Vehicles and fed them to the network for classification of the color.\\
I received a 75.836\% precision on this feature.

\subsection{Experiment 7}
I now wanted to check whether fine-tuning a pretrained ResNet-50 \cite{resnet} would encourage transfer-learning and help to improve the classification accuracy.
I managed to improve the accuracy up to 79.348\%.

\subsection{Experiment 8}
Now I trained (from scratch) a ResNet-50 network for classifying the Subclass fine-grained feature of Small Vehicles only (which differs from Large Vehicle subclasses).
The network however received a disappointing 51.399\% precision on this feature.

\subsection{Experiment 9}
I now repeated the exact previous experiment but with a pretrained ResNet-50.\\
Just like in the previous experiments the pretrained network beat the other fully trained network, but with a small gap. The network reached 52.710\% precision on this feature.

\subsection{Experiment 10}
This time I fine-tuned a pretrained ResNet-50 on the Large Vehicle fine-grained Subclass feature. Surprisingly, the model reached accuracy of 45.283\%, despite having a relatively small ($\approx$500) train examples.

\section{Experiments Conclusions}
\subsection{Coarse grained classes}
The F-RCNN \cite{fasterrcnn} network have successfully learned to detect the Small Vehicle class with Average Precision of 0.90\%. It also succeed to detect Large Vehicle class with 0.75\% AP in the first experiment despite having only hundreds of samples of it. The accuracy decreased to 0.54\% when more classes and high resolution images were added. The decrease in the results can be explained in several ways. First, the network may have confused the Large Vehicle with other classes, such as Solar Panel. Second, it is reasonable that the down scaling process of the high resolution images added even smaller and perhaps skewed objects, which were difficult for the network to correctly detect and classify. This actually affects all the results altogether.\\
The network detected Solar Panel class with 0.45\% AP, which is in average almost as the size of the Small Cars, but appears fifth times less than them in the dataset. Utility Pole class was detected with 0.23\% AP, despite it's small size, probably because it had around 6000 tagged instances in the dataset.

\subsection{Fine grained classes}
In all the experiments fine-tuning the ResNet-50 \cite{resnet} yielded better results than training it from scratch.
The network successfully classified the Color feature with 79.34\% (Large and Small Vehicles combined), the Subclass feature of Small Vehicles with 52.71\% and the Subclass feature of Large Vehicles with 45.28\%. The difference between the pretrained and fully trained networks can probably be explained due to the relatively small object instances fed to the network. The ImageNet \cite{imagenet} dataset, on which the pretrained network is trained on, consists of 14 million images, in contrast with several thousands we got here. \\
The difference between the Color and Subclass accuracies can also be explained due to the difficulty of learning characteristics and nuances between different cars subclasses, unlike learning colors, which usually fill most of the image.

\section{Further Work}
I did not have time to properly incorporate some $\approx$100 high resolution images to the training, as they did not fit into GPU memory. 
The problem can be solved by cropping each image and using Non Maximum Suppression algorithm to omit duplicated predictions from the calculations.
\\
I also did not have enough time to train the ResNet-50 network on all the fine-grained features and calculate a cumulative score for the fine-grained task which takes into account all the Cohen Kappas' scores of all the features, but only managed to proof the feasibility of this task by training on the Color and Subclass features.

\begin{thebibliography}{9}
	\bibitem{vott} 
	Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos.
	\\\texttt{https://github.com/Microsoft/VoTT}
	
	\bibitem{dota} 
	Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang.
	\textit{DOTA: A Large-scale Dataset for Object Detection in Aerial Images}. 
	arXiv:1711.10398v2 [cs.CV], 2018.
	
	\bibitem{resnet} 
	Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.
	\textit{Deep Residual Learning for Image Recognition}. 
	10.1109/CVPR.2016.90, 2016.
	
	\bibitem{imagenet} 
	 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei.
	\textit{ImageNet: A large-scale hierarchical image database}. 
	10.1109/CVPR.2009.5206848, 2009.
	
	\bibitem{pascalvoc} 
	Mark Everingham, Luc Gool, Christopher K. Williams, John Winn, Andrew Zisserman.
	\textit{The Pascal Visual Object Classes (VOC) Challenge}. 
	10.1007/s11263-009-0275-4, 2010.
	
	\bibitem{fasterrcnn} 
	S. Ren, K. He, R. B. Girshick, and J. Sun.
	proposal networks.
	\textit{Faster R-CNN: towards real-time object detection with region}. 
	10.1109/TPAMI.2016.2577031, 2017.
	
	\bibitem{pytorch} 
	PyTorch is a deep learning framework for fast, flexible experimentation.
	\\\texttt{https://pytorch.org/}
	
	\bibitem{jianweiyang} 
	Jianwei Yang's F-RCNN PyTorch implementation
	\\\texttt{https://github.com/jwyang/faster-rcnn.pytorch}
	
\end{thebibliography}

\end{document}